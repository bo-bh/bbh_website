---
author: Borhane Blili-Hamelin
categories:
- Research
- Work in progress
date: "2023-11-08"
draft: false
excerpt: Ongoing research project on the potential of red-teaming for empowering communities to recognize and prevent generative AI harms

layout: single
links:
- icon: door-open
  icon_pack: fas
  name: August 2023 Op-Ed
  url: https://techpolicy.press/can-we-red-team-our-way-to-ai-accountability/
- icon: door-open
  icon_pack: fas
  name: Data & Society Policy Brief
  url: https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/

subtitle: Research partnership between ARVA and Data & Society
tags:
- research
title: Red-Teaming in the Public Interest
---
A research partnership between ARVA and Data & Society investigating the potential of red-teaming for empowering communities to recognize and prevent generative AI harms. ARVA's mission is to empower communities to recognize, diagnose, and manage vulnerabilities in AI systems. 

ARVA believes that red-teaming generative AI is a major emergent use case for knowledge sharing about AI vulnerabilities. It is also a major potential source of public knowledge about generative AI vulnerabilities. This is why we are proud to be community partners in the groundbreaking White House-supported [DEF CON 31 Generative Red Team event co-organized by AI Village, Humane Intelligence, and Seed AI](https://www.hackthefuture.com). 

The surge of interest in red-teaming as an approach to discovering ethics and safety flaws in generative AI systems marks a crucial moment for AI risk management and governance. Amidst this growing interest, we see a need for investigating the place of red-teaming in the emergent ecosystem of discovery, disclosure, and mitigation of harmful flaws in generative AI systems. To this end, we partnered with Data & Society on a major research project that will examine this problem through ethnography and desk research.

This project is in part supported by a 2023-2024 Magic Grant from [The Brown Institute for Media Innovation](https://brown.columbia.edu/project-type/magic-grants/).

For early outputs from this ongoing project, see our [August 2023 op-ed on Tech Policy Press](https://techpolicy.press/can-we-red-team-our-way-to-ai-accountability/), and our [October 2023 Data & Society Policy Brief](https://datasociety.net/library/ai-red-teaming-is-not-a-one-stop-solution-to-ai-harms-recommendations-for-using-red-teaming-for-ai-accountability/).

Thumbnail photo by [Ariel Lu](https://www.ariellu.com) on [Unsplash](https://unsplash.com/photos/a-close-up-of-a-multicolored-object-on-a-black-background-DtJ9q3Mu0Pw). 