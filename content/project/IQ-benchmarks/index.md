---
author: Borhane Blili-Hamelin
categories:
- Research
- Paper
date: "2022-09-02"
draft: false
excerpt: A research paper drawing on feminist scholarship about IQ to shed lights on overlooked areas of ethical risk for ML benchmarks

layout: single
links:
- icon: door-open
  icon_pack: fas
  name: Paper
  url: https://dl.acm.org/doi/fullHtml/10.1145/3593013.3593996

subtitle: FAccT 2023 paper co-authored with Leif Hancox-Li
tags:
- research
title: Making Intelligence, Ethics, IQ, and ML Benchmarks
---
A FAccT 2023 paper co-authored with Leif Hancox-Li that draws on feminist scholarship about IQ to shed lights on overlooked areas of ethical risk for ML benchmarks. [Read the paper: https://dl.acm.org/doi/fullHtml/10.1145/3593013.3593996.](https://dl.acm.org/doi/fullHtml/10.1145/3593013.3593996)

(*Abstract*) In recent years, ML researchers have wrestled with defining and improving machine learning (ML) benchmarks and datasets. In parallel, some have trained a critical lens on the ethics of dataset creation and ML research. In this position paper, we highlight the entanglement of ethics with seemingly “technical” or “scientific” decisions about the design of ML benchmarks. Our starting point is the existence of multiple overlooked structural similarities between human intelligence benchmarks and ML benchmarks. Both types of benchmarks set standards for describing, evaluating, and comparing performance on tasks relevant to intelligence—standards that many scholars of human intelligence have long recognized as value-laden. We use perspectives from feminist philosophy of science on IQ benchmarks and thick concepts in social science to argue that values need to be considered and documented when creating ML benchmarks. It is neither possible nor desirable to avoid this choice by creating value-neutral benchmarks. Finally, we outline practical recommendations for ML benchmark research ethics and ethics review.

Thumbnail photo by [Khyati Trehan](https://khyatitrehan.com) on [Unsplash](https://unsplash.com/photos/rXy5Zlmw3qY).